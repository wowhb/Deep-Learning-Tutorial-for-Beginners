{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f7e7923-bf6d-4e89-a4d2-601f11648598",
    "_uuid": "a5f2d3f6246fe1c767e3b9469c17d1bf09c04708"
   },
   "source": [
    "<font color='green'>\n",
    "    \n",
    "**Deep Learning Tutorial for Beginners 을 시작 해보겠습니다.** \n",
    "    \n",
    "* 이 노트북은 Data ScienceTutorial for Beginners(https://github.com/wowhb/Data-Science-Tutorial-for-Beginners) 를 이어서 작성하는 것으로 Deep Learning Tutorial 이라고 생각하시면 됩니다. kaggle 의 DATAI팀의 작업물은 번역 및 보완 한 노트북입니다.\n",
    "    \n",
    "* 이번 노트북에서 기본적인 모든 것을 하나하나 설명하려고 합니다.     \n",
    "* 키워드를 중심으로 정의합니다.\n",
    "* 이 튜토리얼의 끝에는 딥러닝에 대한 충분한 정보가 수록되어 있어 더 깊이 학습할 수 있습니다.\n",
    "* 내용을 살펴보겠습니다. 먼저 컨텐츠의 구성 목차를 보고 시작하세요.\n",
    "\n",
    "<font color='red'>\n",
    "<br>Content:\n",
    "    \n",
    "* [Introduction](#1)\n",
    "* [Overview the Data Set](#2)\n",
    "* [Logistic Regression](#3)\n",
    "    * [Computation Graph](#4)\n",
    "    * [Initializing parameters](#5)\n",
    "    * [Forward Propagation](#6)\n",
    "        * Sigmoid Function\n",
    "        * Loss(error) Function\n",
    "        * Cost Function\n",
    "    * [Optimization Algorithm with Gradient Descent](#7)\n",
    "        * Backward Propagation\n",
    "        * Updating parameters\n",
    "    * [Logistic Regression with Sklearn](#8)\n",
    "    * [Summary and Questions in Minds](#9)\n",
    "    \n",
    "* [Artificial Neural Network](#10)\n",
    "    * [2-Layer Neural Network](#11)\n",
    "        * [Size of layers and initializing parameters weights and bias](#12)\n",
    "        * [Forward propagation](#13)\n",
    "        * [Loss function and Cost function](#14)\n",
    "        * [Backward propagation](#15)\n",
    "        * [Update Parameters](#16)\n",
    "        * [Prediction with learnt parameters weight and bias](#17)\n",
    "        * [Create Model](#18)\n",
    "    * [L-Layer Neural Network](#19)\n",
    "        * [Implementing with keras library](#22)\n",
    "* Time Series Prediction: https://www.kaggle.com/kanncaa1/time-series-prediction-with-eda-of-world-war-2\n",
    "* [Artificial Neural Network with Pytorch Library](#23)\n",
    "* [Convolutional Neural Network with Pytorch Library](#24)\n",
    "* [Recurrent Neural Network with Pytorch Library](#25)\n",
    "* [Conclusion](#20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "036bf2c0-c146-4c70-b29d-206db0fe91b0",
    "_uuid": "01d54760756dd2bc5c2a309f2862833c079a3303"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "# INTRODUCTION\n",
    "* **Deep learning:** One of the machine learning technique that learns features directly from data. 정의 그대로 데이터에서 직접 기능을 학습하는 머신러닝 기법 중 하나입니다.  \n",
    "  \n",
    "* **Why deep learning:** 데이터의 양이 증가하면, 머신러닝은 성능 면에서 불충분하고 딥러닝은 정확성과 같은 성능면에서 더 나은 성능을 제공합니다.\n",
    "<a href=\"http://ibb.co/m2bxcc\"><img src=\"http://preview.ibb.co/d3CEOH/1.png\" alt=\"1\" border=\"0\"></a>\n",
    "* **What is amounth of big:** 대답하기 어렵지만 직관적으로 100만 개의 샘플이 \"빅데이터\"라고 말하기에 충분합니다.  \n",
    "* **Usage fields of deep learning:** 음성 인식, 이미지 분류, 자연어 처리(nlp) 또는 추천 시스템 등등이 있습니다.\n",
    "* **What is difference of deep learning from machine learning:** \n",
    "    * 머신러닝은 딥러닝을 커버합니다.\n",
    "    * 머신러닝에서는 feature 가 수동으로 학습을 실시합니다.\n",
    "    * 한편, 딥러닝은 데이터로부터 직접 feature를 학습합니다.\n",
    "<a href=\"http://ibb.co/f8Epqx\"><img src=\"http://preview.ibb.co/hgpNAx/2.png\" alt=\"2\" border=\"0\"></a>\n",
    "\n",
    "<br>데이터를 먼저 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6fe1e8d5-b36d-4e39-9a9b-34618b5e275e",
    "_uuid": "79f18357b846d2cd91e0f7b2389e1dba8097cbdb"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\BIN\\\\Deep Learning Tutorial for Beginners'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() #제가 작업하고 있는 경로를 먼저 파악합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Deep Learning Tutorial for Beginners.ipynb', 'X.npy', 'X.npy.zip', 'Y.npy']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.npy.zip\n",
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\X.npy.zip\n",
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\X.npy.zip\n"
     ]
    }
   ],
   "source": [
    "# 제가 불러올 데이터의 경로를 파악합니다. \n",
    "print('X.npy.zip')\n",
    "print(os.path.realpath('X.npy.zip'))\n",
    "print(os.path.abspath('X.npy.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list: ['.ipynb_checkpoints', 'Deep Learning Tutorial for Beginners.ipynb', 'X.npy', 'X.npy.zip', 'Y.npy']\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "print (\"file_list: {}\".format(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\Deep Learning Tutorial for Beginners.ipynb\n",
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\X.npy.zip\n",
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\Y.npy\n",
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\.ipynb_checkpoints\\Deep Learning Tutorial for Beginners-checkpoint.ipynb\n",
      "C:\\Users\\BIN\\Deep Learning Tutorial for Beginners\\X.npy\\X.npy\n"
     ]
    }
   ],
   "source": [
    "for dirname,_,filenames in os.walk('C:\\\\Users\\\\BIN\\\\Deep Learning Tutorial for Beginners') :\n",
    "    for filename in filenames :\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Overview the Data Set\"></a> <br>\n",
    "# Overview the Data Set\n",
    "* 본 튜토리얼에는 \"sign language digits data set\"를 사용할 것이다.  \n",
    "* 이 데이터에는 2062개의 수화 숫자 이미지가 있습니다.  \n",
    "* 알다시피 숫자는 0에서 9까지 입니다. 따라서 10개의 독특한 표지판이 있습니다.  \n",
    "* 튜토리얼의 시작 부분에서는 단순성을 위해 sign 0과 1만 사용할 것입니다.  \n",
    "* 데이터에서 0은 index 204와 408 사이에 있습니다. 0의 index의 개수는 205입니다.  \n",
    "* 또한 1은 index 822와 1027 사이에 있습니다. 1개의 sign은 206개입니다. 따라서 각 classes(레이블)에서 205개의 샘플들을 사용할 것입니다.\n",
    "* 참고: 실제로 205개의 샘플은 딥러닝에 매우 적습니다. 하지만 이것은 튜토리얼이기 때문에 그다지 중요하지 않습니다.\n",
    "* X 및 Y array를 준비하겠습니다. \n",
    "* X는 이미지 array(0과 1의 sign)이고 Y는 레이블 배열(0과 1)이다.\n",
    "\n",
    "\n",
    "#### 간단하게 직관적으로 설명해 보자면 X 이미지 (레이블이 0과 1뿐인) 를 가지고 훈련을 시키고 이미지를 보고 그 수화이미지가 가르키는게 0인지 1인지를 분류하는 작업이라고 생각하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "4768ce70-3e7f-4ac9-8764-642e88006b77",
    "_uuid": "d6b38399b27c2d723750c0b4f8787a7d6d0025ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data set\n",
    "# 0은 index 204와 408 사이에 있습니다\n",
    "# 1은 index 822와 1027 사이에 있습니다 \n",
    "# 하나씩 임의로 불러와서 확인해 보겠습니다\n",
    "x_l = np.load('X.npy\\X.npy')\n",
    "Y_l = np.load('Y.npy')\n",
    "img_size = 64\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(x_l[260].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(x_l[900].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(x_l[204].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(x_l[1027].reshape(img_size, img_size))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c9cf8f2-f318-4f69-b92e-9c1bfe78c2c7",
    "_uuid": "c12d1854c6f5fb43c8f081083a441585ed48dc93"
   },
   "source": [
    "* image array를 생성하기 위해서 sign 0 과 sign 1 array 를 연결해야 합니다.\n",
    "* 그런다음 0 sign 의 image 에는 0이라는 레이블을 1 sign 의 image 에는 1이라는 레이블을 부여해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "88a9b18b-12c7-46d6-8370-e2f02b55dd7a",
    "_uuid": "ad417f6189b0d9476388d92b6be09c887f7f1a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (410, 64, 64)\n",
      "Y shape:  (410, 1)\n"
     ]
    }
   ],
   "source": [
    "# 행 축을 따라 array 순서를 결합합니다\n",
    "X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) \n",
    "# index 0부터 204 까지는 label 0 을 205 부터 410 까지는 label 1 을 부여합니다.\n",
    "z = np.zeros(205)\n",
    "o = np.ones(205)\n",
    "Y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\n",
    "print(\"X shape: \" , X.shape)\n",
    "print(\"Y shape: \" , Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6024477-cf34-40f9-b079-8388b5146e1b",
    "_uuid": "a999c02e74af603d65a4442842d739aa351f593c"
   },
   "source": [
    "* X의 shape은 (410, 64, 64)\n",
    "    * 410은 우리가 410개의 이미지를 가지고 있다는 것을 의미합니다. (0과 1개의 sign)\n",
    "    * 64는 이미지 크기가 64x64(64x64픽셀)임을 의미합니다\n",
    "* Y의 shape (410,1))\n",
    "    *  410은 우리가 410개의 라벨을 가지고 있다는 것을 의미합니다. (0과 1)\n",
    "* X와 Y를 train sets 와 test sets 로 나누겠습니다.\n",
    "    * test_size = test = 15%, train = 75%\n",
    "    * random_state = 랜덤화하면서 동일한 시드 사용합니다. train_test_split를 반복해서 부르면 항상 같은 train과 test분포를 만들어 낸다는 뜻인데, 이는 우리가 같은 random_state를 가지고 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "00db1f03-75d3-48dc-a6fb-824a58790d42",
    "_uuid": "82869efdb6890ede1899c1bc8c90fa8b1caceec3"
   },
   "outputs": [],
   "source": [
    "# 그런 다음 x_train, y_train, x_test, y_test 들 array를 생성합니다\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bb55e07-cabb-46ce-b21a-fe4a7b34995f",
    "_uuid": "e294dc952e7ba34569162a8965680ead8edd2834"
   },
   "source": [
    "* 이제 우리는 3차원 input array(X)을 가지고 있으므로 첫 번째 딥러닝 모델의 입력으로 사용하기 위해서는 flatten(2D)을 만들어야 합니다.\n",
    "* 우리의 레이블 array(Y)은 이미 flatten(2D) data 이기 때문에 그대로 둡니다.\n",
    "* X data 를 flatten 해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "f5937123-1e16-4036-844b-f498ed42e504",
    "_uuid": "f8b7203144bc85873d960f765e2bb3e711b57f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train flatten (348, 4096)\n",
      "X test flatten (62, 4096)\n"
     ]
    }
   ],
   "source": [
    "X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\n",
    "print(\"X train flatten\",X_train_flatten.shape)\n",
    "print(\"X test flatten\",X_test_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dab40a5b-e319-4807-a608-12065d860847",
    "_uuid": "cb26fc242ffcae8bf8f164d32dc9c5436e8312fc"
   },
   "source": [
    "* 보시다시피, 우리는 348개의 이미지를 가지고 있고 각각의 이미지는 4096개의 픽셀을 가지고 있습니다.\n",
    "* 또한 우리는 62개의 이미지를 가지고 있고 각각의 이미지는 4096개의 픽셀을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "ad9bee66-78f1-44ec-a114-465356b9cc7d",
    "_uuid": "88eef1b839ee51234a53da3ccb1e36a2e5e9a0e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (4096, 348)\n",
      "x test:  (4096, 62)\n",
      "y train:  (1, 348)\n",
      "y test:  (1, 62)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train_flatten.T\n",
    "x_test = X_test_flatten.T\n",
    "y_train = Y_train.T\n",
    "y_test = Y_test.T\n",
    "print(\"x train: \",x_train.shape)\n",
    "print(\"x test: \",x_test.shape)\n",
    "print(\"y train: \",y_train.shape)\n",
    "print(\"y test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0dfdae4c-5c3e-4ebd-8714-2ecf974ef2fa",
    "_uuid": "ed7b18eea8062e401823686bb2f672e1c548fac0"
   },
   "source": [
    "<font color='purple'>\n",
    "    \n",
    "지금까지 진행한 사항:  \n",
    "    \n",
    "* sign 0과 sign1 을 선택하고 그에 대응하는 label 0 과 1 을 연결시켰습니다.\n",
    "* train set과 test set으로 나누고 flatten data 로 변형시켰습니다.\n",
    "* 최종적으로 우리의 input과 output 은 다음 그림으로 표현해보겠습니다:\n",
    "<a href=\"http://ibb.co/bWMK7c\"><img src=\"http://image.ibb.co/fOqCSc/3.png\" alt=\"3\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8b5f8812-f21c-4936-91b9-d33048dec40b",
    "_uuid": "5f037b5d3f44a9bf139dddb7946c0b33cf7d0298",
    "collapsed": true
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "# Logistic Regression\n",
    "* 이진분류(0과 1 출력)를 이야기할 때 가장 먼저 떠오르는 것은 로지스틱 회귀입니다.\n",
    "* 그러나 딥러닝 튜토리얼에서는 로지스틱 회귀 분석을 어떻게 해야 할까요?\n",
    "* 해답은 로지스틱 회귀는 사실 매우 단순한 신경망이라는 것입니다.\n",
    "* 그런데 신경망과 딥러닝은 같은 것입니다. \"deep\"과 같은 용어에 대해 자세히 설명할 것입니다.\n",
    "* 로지스틱 회귀(단순 딥러닝)를 이해하기 위해 먼저 Computation Graph를 학습해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9db2d7f4-0393-47d2-bbf3-88600177b4d1",
    "_uuid": "e2f49564d7d487417546d3b1ce94bc63e6394632"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "##  Computation Graph\n",
    "* Computation graphs 는 수학적 표현에 대해 생각해보는 좋은 방법입니다.\n",
    "* 수학적 표현을 시각화하는 것과 같습니다.\n",
    "* 예를들자면   \n",
    "  $$c = \\sqrt{a^2 + b^2}$$\n",
    "* 보다시피 우리는 수학을 그래프로 표현합니다.\n",
    "<a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/hWn6Lx/d.jpg\" alt=\"d\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c968bd6-39d6-4497-add1-6fd1eb331659",
    "_uuid": "5060aff503466a5c54bcf76a545484abda032e03"
   },
   "source": [
    "* 이제 로지스틱 회귀 분석의 computation graph 를 살펴봅시다.\n",
    "<a href=\"http://ibb.co/c574qx\"><img src=\"http://preview.ibb.co/cxP63H/5.jpg\" alt=\"5\" border=\"0\"></a>\n",
    "    * Parameters 는 weight 와 bias 입니다.\n",
    "    * Weights: coefficients(계수) of each pixels\n",
    "    * Bias: intercept\n",
    "    * z = (w.t)x + b  => z equals to (transpose of weights times input x) + bias \n",
    "    * 다시말하자면 => z = b + px1*w1 + px2*w2 + ... + px4096*w4096\n",
    "    * y_head = sigmoid(z)\n",
    "    * Sigmoid function은 z를 0과 1 사이로 만들 확률입니다. computation graph에서 sigmoid 함수를 볼 수 있습니다.\n",
    "* Why we use sigmoid function?\n",
    "    * 확률론적 결과를 제공합니다.\n",
    "    * gradient descent algorithm에서 사용할 수 있습니다 (곧 알게 될것입니다.)\n",
    "* 예를 들어보겠습니다:\n",
    "    * z = 4를 찾아 z를 sigmoid 함수에 넣었다고 가정하겠습니다. 결과(y_head)는 거의 0.9입니다. 그것은 우리의 분류 결과가 90% 확률로 1이라는 것을 의미합니다.\n",
    "* 이제 처음부터 computation graph의 각 구성 요소를 하나하나 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9ec7e1d-186b-4911-be80-bc856f43b689",
    "_uuid": "40c4a1af2372960d36b6649f5baf4915e9e16738"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## Initializing parameters\n",
    "* 알다시피 입력은 4096픽셀(x_train의 각 이미지)\n",
    "* 각 각의 픽셀은 자체의 weight 가 있습니다\n",
    "* 첫 번째 단계는 각각의 픽셀에 각각의 weight를 곱하는 것이다.\n",
    "* 문제는 가중치의 초기 값은 무엇일까요?\n",
    "    * 인공신경망에서 설명할 기법도 있지만, 이때의 초기 weight는 0.01입니다\n",
    "    * weight는 0.01인데 weight array shape은? 로지스틱 회귀 분석의 computation graph에서 이해한 바와 같이 (4096,1) 입니다\n",
    "    * 또한 초기 bias는 0입니다.\n",
    "* 코드를 좀 써보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "74d461fc-4aa9-4b76-bd26-43551cef138b",
    "_uuid": "f3be95b8ca86fea08336badabf563b5f8f59a142"
   },
   "outputs": [],
   "source": [
    "# 정의의 간단한 설명 및 예\n",
    "def dummy(parameter):\n",
    "    dummy_parameter = parameter + 5\n",
    "    return dummy_parameter\n",
    "result = dummy(3)     # result = 8\n",
    "\n",
    "# 매개 변수를 초기화할 수 있습니다\n",
    "# So what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d71d5b420ee146833d479781ea889eeebcb74e47"
   },
   "outputs": [],
   "source": [
    "#w,b = initialize_weights_and_bias(4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1fbf27d5-1f4b-4b94-a757-99e2becf54a0",
    "_uuid": "dde6fab55966b2de3dc808e03613df9bea7bb0d2"
   },
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "## Forward Propagation\n",
    "* 픽셀부터 비용까지 모든 단계를 forward propagation 라 합니다.\n",
    "    * z = (w.T)x + b => 이 방정식에서 우리는 픽셀 array인 x를 알고 있고, w(weight)와 b(bias)를 알고 있으므로 나머지는 계산입니다.(T is transpose)\n",
    "    * 그런 다음 y_head(확률)를 반환하는 sigmoid 함수에 z를 넣습니다. 헷갈리신다면 가서 computation graph를 다시 보십시오. 또한 sigmoid 함수의 방정식은 computation graph에 있습니다.\n",
    "    * 그런 다음 손실(Loss) 함수를 계산합니다.\n",
    "    * 비용(Cost) 함수는 모든 손실(오류)의 합입니다.\n",
    "    * z와 입력 매개 변수로 z를 가져오고 y_head(확률)를 반환하는 쓰기 sigmoid 정의(방법)로 시작해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "697be401-792b-46c0-8fe6-79cd65110419",
    "_uuid": "e024479bce9ce2022f65ffd586a6c29b124e7ab5"
   },
   "outputs": [],
   "source": [
    "# calculation of z\n",
    "#z = np.dot(w.T,x_train)+b\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "da7e0244449200eaf382507806f01901f1d281c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_head = sigmoid(0)\n",
    "y_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "571dc02a-b25d-4726-ad2b-9661ae6783e2",
    "_uuid": "d67ca31d01cc8f03fdd862789454e4acf1b2033b"
   },
   "source": [
    "* Sigmoid method를 작성하고 y_head를 계산하면서 손실(오류) 기능이란 무엇인지 알아보겠습니다\n",
    "* 예를 들어, 하나의 이미지를 input으로 넣은 후 weight를 곱하고 bias를 추가하여 z를 찾도록 합니다. 그런 다음 z를 sigmoid method에 넣어 우리가 y_head를 찾을 수 있도록 합니다. 그 후 y_head가 0.5보다 큰 0.9가 되었기 때문에 이미지 하나가 sign하는 것이 우리의 예측입니다. 그런데 우리의 예측이 맞느냐, 틀리느냐, 어떻게 확인하느냐. 정답은 손실(loss) function입니다::\n",
    "    * 로그 loss(오류) function의 수학적 표현은 다음과 같습니다: \n",
    "    <a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/eC0JCK/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"></a>\n",
    "    * 잘못 예측하면 손실(오류)이 커집니다. **DENKLEM DUZELTME**\n",
    "        * Example: 우리의 실제 이미지는 sign 1이고 그 label은 1(y = 1)이며, 그 다음 우리는 예측 y_head = 1을 만듭니다. y와 y_head를 손실(loss) 방정식에 넣었을 때 결과는 0이게 됩니다. 우리는 정확한 예측을 하기 때문에 우리의 loss는 0입니다. 그러나 y_head = 0과 같이 잘못된 예측을 하면 loss(오류)은 무한대가 되게 됩니다.\n",
    "* 그 후 비용함수는 손실함수의 합입니다. 각각의 이미지는 손실 기능을 만듭니다. 비용 함수는 각 입력 이미지에 의해 생성되는 손실 함수의 합입니다.\n",
    "* forward propagation 를 구현해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "adbc7d22-8bba-48c1-b754-75c9d4f0543c",
    "_uuid": "447f1ee51819fdfeba0e1ce0b03449cb549510ed"
   },
   "outputs": [],
   "source": [
    "# Forward propagation steps:\n",
    "# find z = w.T*x+b\n",
    "# y_head = sigmoid(z)\n",
    "# loss(error) = loss(y,y_head)\n",
    "# cost = sum(loss)\n",
    "def forward_propagation(w,b,x_train,y_train):\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z) # probabilistic 0-1\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4700f54-c2c5-4a53-ad1f-7fd63f5cd2e9",
    "_uuid": "e577873215128f94fea2eb328de90a4f32b7f803"
   },
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "##  Optimization Algorithm with Gradient Descent\n",
    "* 이제 우리는 오류인 loss 와 cost 가 무엇인지 알게 되었습니다\n",
    "* 즉, cost 가 높으면 우리가 잘못 예측한다는 것을 의미하기 때문에 cost를 줄여야 합니다\n",
    "* 모든 것은 weight 와 bias 를 초기화하는 것으로부터 시작됨을 생각해 봅시다. 즉, 모든것은 그것들에 게 달려있습니다\n",
    "* cost 를 낮추기 위해서는 weight 와 bias 를 조절하고 갱신해야 합니다\n",
    "* 다시 말해, 우리의 모델은 비용 기능을 최소화하는 모수 weight와 bias에 대해 배울 필요가 있습니다. 이 기법을 gradient descent 라고 합니다.\n",
    "* 예를 들어보겠습니다:\n",
    "    * 우리는 w = 5와 bias = 0을 가지고 있습니다(그러므로 일단은 bias은 무시합니다). 그리고 나서 우리는 Forward propagation를 하고 우리의 cost function은 1.5입니다다.\n",
    "    * 이것을 보세요. (빨간줄)\n",
    "    <a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/dAaYJH/7.jpg\" alt=\"7\" border=\"0\"></a>\n",
    "    * 그래프에서 볼 수 있듯이, 우리는 최소 비용 함수가 아닙니다. 그러므로 우리는 최소한의 비용을 지불하는 것이 목적입니다. 그러면 weight를 업데이트 합시다. (:= 업데이트 중이라는 기호로 사용하겠습니다)\n",
    "    * w := w - step. 문제는 이 단계가 무엇인가 하는 것입니다. slope1을 보십시오. 최소점을 찾기 위해서, 우리는 slope1을 사용할 수 있습니다. 그럼 slope1 = 3이라고 하고 weight를 업데이트해보죠. w := w - slope1 => w = 2.\n",
    "    * 이제 우리 weight w는 2입니다. 기억하시겠지만, 우리는 Forward propagation를 통해 비용 기능을 다시 찾아야 합니다. \n",
    "    * w = 2를 사용한 Forward propagation에 따라 비용 함수는 0.4라고 합시다. 흠, 우리의 비용 기능이 감소하기 때문에 우리는 올바른 길을 가고 있다고 볼 수 있습니다. 우리는 비용 = 0.4인 비용 함수에 대한 새로운 값을 가지고 있지만 그게 충분한지 모르겠습니다 한 걸음 더 해보겠습니다.\n",
    "    * Slope2 = 0.7 그리고 w = 2. weight w : = w - step(slope2) => w = 1.3 로 업데이트 하고 새로운 비용함수를 찾아보겠습니다\n",
    "    * w = 1.3과 우리의 비용 = 0.3으로 한 번 더 Forward propagation를 만드세요. 괜찮아 보이는데, 충분한가요? 아니면 한 번 더 가야 할까요? 대답은 모른다는 겁니다, 한번 더 해보죠.\n",
    "    * Slope3 = 0.01 그리고 w = 1.3. Updating weight w := w - step(slope3) => w = 1.29 ~ 1.3. 그래서 우리는 최소한의 비용 기능을 찾기 때문에 weight는 변하지 않습니다.\n",
    "    * 모든 것이 좋아 보이는데 어떻게 slope를 찾을 수 있을까요? 고등학교나 대학교 때 기억한다면, 주어진 지점에서 기능(비용 함수)의 기울기를 찾기 위해 우리는 주어진 지점에서 미분을 취합니다. 또한 당신은 우리가 경사를 찾을 수 있지만 그것이 어디로 가는지 어떻게 알 수 있는지 물어볼 수 있습니다. 최소로 가는 대신에 더 높은 비용 값으로 갈 수 있다고 말할 수 있습니다. 경사(미분값)는 발걸음과 방향을 동시에 주는 것입니다다. 그러므로 걱정하지 마십시오.\n",
    "    * 업데이트 방정식은 이렇습니다. 비용함수(weight과 bias)가 있다고 합시다. weight와 bias에 따라 비용 함수의 미분함수를 취합니다. 그런 다음 α 학습 비율로 곱하십시오. 그런 다음 weight를 업데이트하십시오. (설명하기 위해 편향은 무시하지만 이 모든 단계는 편향에 적용됨).\n",
    "    <a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/hYTTJH/8.jpg\" alt=\"8\" border=\"0\"></a>\n",
    "    * 이제, 제가 말한 적 없는 학습율이 얼마인지 묻고 있을 것입니다. 그것은 학습율을 결정하는 매우 간단한 용어입니다. 빨리 배우는 것과 배우지 않는 것 사이에는 절충이 있습니다. 예를 들어 당신은 지금 파리에 있고 마드리드에 가고 싶어 한다고 합시다. 스피드가 작으면 마드리드에 아주 천천히 갈 수 있고 시간이 너무 오래 걸립니다. 반면에, 만약 당신의 속도가 크다면, 당신은 매우 빨리 갈 수 있지만 아마도 당신은 추락해서 마드리드에 가지 않을 수도 있습니다. 그러므로 우리는 우리의 속도(학습 속도)를 현명하게 선택할 필요가 있습니다.\n",
    "    * 학습율은 선택과 조정이 필요한 하이퍼 파라미터라고도 합니다. 다른 하이퍼 파라미터와 함께 인공신경망에서 더 자세히 설명하겠습니다. \n",
    "  \n",
    "* 나는 이제 당신은 (weight과 bias에서 cost에 이르기까지) Forward propagation와 (cost에서 weight과 bias으로) backwork propagation의 논리를 이해했다고 생각합니다. 또한 경사하강도 배웁니다. 코드를 구현하기 전에 weight와 bias에 따라 cost 함수의 미분 요소를 취하는 방법에 대해 한 가지 더 배울 필요가 있습니다. 그것은 파이썬이나 코딩과는 관련이 없습니다. 그것은 순수한 수학입니다. \n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "2b74bb7c-bcbe-4865-ac61-25ced274e7de",
    "_uuid": "088f192f99d2a31f041116c893ec713ef3b4fe43"
   },
   "outputs": [],
   "source": [
    "# backward propagation 시 foward propagation 에서 발견된 y_head 를 사용합니다\n",
    "# 그러므로backward propagation을 쓰지 말고, foward propagation와 backward propagation를 결합하겠습니다\n",
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d82dbae8-d11e-4ea8-bbfe-545fee3166b3",
    "_uuid": "9e4d028259897e1565341736d58e46f70ea6b312"
   },
   "source": [
    "* 여기까지 학습한 것\n",
    "    * 파라미터 초기화\n",
    "    * forward propagation 및 cost function 으로 cost 찾기\n",
    "    * parameters (weight and bias) 업데이트. \n",
    "    \n",
    "* 이제 구현해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "0940e18d-636d-4503-a2c6-1994024726fe",
    "_uuid": "31299bda686ae2ab157ea18df70e4741e5225345"
   },
   "outputs": [],
   "source": [
    "# Updating(learning) parameters\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters 는 number_of_iterarion times 입니다\n",
    "    for i in range(number_of_iterarion):\n",
    "        # forward 와 backward propagation 그리고 cost 와 gradients 찾습니다\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # weights 와 bias 를 업데이트 합니다\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list\n",
    "#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "202a05c6-2187-40eb-9733-e4da792f04b6",
    "_uuid": "1892ccefd7debe1e9f59b500873b7afded3a4a01"
   },
   "source": [
    "* 여기까지 우리는 우리의 parameters 를 배웁니다. 그건 우리가 data를 적합시킨다는 것을 의미합니다\n",
    "* 예측을 위한 parameter가 있습니다 그러므로 예측해보겠습니다\n",
    "* 예측 단계에서는 x_test 를 입력으로 하고 사용하면서 foward propagation 을 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "1fd35d0f-e989-49fa-9bcb-1769d92a5ab4",
    "_uuid": "c9b80f081f49a722818cfebfc01e8b8dc69db9c1"
   },
   "outputs": [],
   "source": [
    " # prediction\n",
    "def predict(w,b,x_test):\n",
    "    # forward propagation를 하기 위해 x_test 가 input 입니다 \n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # z가 0.5보다 크면 sign 1(y_head=1)이 예측됩니다.\n",
    "    # z가 0.5보다 작으면 sign 0(y_head=0)이 예측됩니다\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce815319-161b-408b-84fe-4bb1fab92d13",
    "_uuid": "40dbb73794b6b742b01e33038284aa9f11cc698c"
   },
   "source": [
    "* 우리는 예측을 만들었습니다.\n",
    "* 이제 그것들을 모두 합치겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "029e2cd3-125b-4ca4-8b94-25d8164ba1a7",
    "_uuid": "81fb6989ff3860d72462f8212b1d00325272a471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 14.014222\n",
      "Cost after iteration 10: 2.544689\n",
      "Cost after iteration 20: 2.577950\n",
      "Cost after iteration 30: 2.397999\n",
      "Cost after iteration 40: 2.185019\n",
      "Cost after iteration 50: 1.968348\n",
      "Cost after iteration 60: 1.754195\n",
      "Cost after iteration 70: 1.535079\n",
      "Cost after iteration 80: 1.297567\n",
      "Cost after iteration 90: 1.031919\n",
      "Cost after iteration 100: 0.737019\n",
      "Cost after iteration 110: 0.441355\n",
      "Cost after iteration 120: 0.252278\n",
      "Cost after iteration 130: 0.205168\n",
      "Cost after iteration 140: 0.196168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 92.816091954023 %\n",
      "test accuracy: 93.54838709677419 %\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # 초기화\n",
    "    dimension =  x_train.shape[0]  #  4096\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # learning rate(학습률) 을 바꾸지 않았습니다\n",
    "    \n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    # Print train/test Errors(loss를 출력)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6603c5a2-0a1b-4e4a-addd-a89be453e6fd",
    "_uuid": "1da3b972bab3207d2ba77f3d21516296f3e9be02"
   },
   "source": [
    "* 우리는 단순 신경망(논리적 회귀)의 이면에 있는 논리와 구현 방법을 배웁니다.\n",
    "* 논리적인 것들을 배웠기 때문에 로지스틱 회귀 분석을 위해 모든 단계를 손으로 실행하는 것보다 쉬운 sklearn 라이브러리를 사용해서 더욱 더 쉽게 해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "119db4ab-f04c-41c0-aef9-728565786e94",
    "_uuid": "ba8b9c960d1735b146fbbad378c90902e7218d59"
   },
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "## Logistic Regression with Sklearn\n",
    "* sklearn library에서는 로지스틱 회귀법을 쉽게 구현할 수 있는 로지스틱 회귀법이 있습니다.\n",
    "* 나는 sklearn에서 로지스틱 회귀의 각 매개변수를 설명하지 않을 것입니다. 그러나 만약 당신이 원한다면 여기서 읽을 수 있습니다 http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* 정확도는 우리가 발견한 것과 다르다. 로지스틱 회귀 분석 방법은 다른 최적화 매개변수나 정규화처럼 사용하지 않는 다양한 feature을 많이 사용하기 때문입니다.\n",
    "* 로지스틱 회귀의 결론을 내리고 인공신경망으로 계속 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "5bc37200-cd53-4da3-a2ef-c630cf1a2d10",
    "_uuid": "a13b1565f8a5ae234eaa9ed6da288103a3579938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.967741935483871 \n",
      "train accuracy: 1.0 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\n",
    "print(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\n",
    "print(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b889d43b-15c9-4f31-a4ff-c51c9b5ac077",
    "_uuid": "3379f61c2fb28d4f6e0e7b55fa31da22bab037a9"
   },
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "## Summary\n",
    "<font color='purple'>\n",
    "    \n",
    "우리가 첫번 째 파트에서 한 것들은 다음과 같습니다:\n",
    "\n",
    "* parameters초기화 즉 weight and bias 초기화\n",
    "* Forward propagation\n",
    "* Loss function\n",
    "* Cost function\n",
    "* Backward propagation (gradient descent)\n",
    "* 학습된 parameters, weight 와 bias 를 사용한 예측\n",
    "* Logistic regression with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a800aa00-7adf-4d10-83c1-3a5221219329",
    "_uuid": "dd0e970ce5194a34ca55b27b6580ef85714e1b92"
   },
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "# Artificial Neural Network (ANN)\n",
    "* 이것은 deep neural network 또는 deep learning 이라고 합니다\n",
    "* **What is neural network:** (신경망이란?) 기본적으로 로지스틱 회귀 분석을 취하여 최소 2회 이상 반복하는 것입니다.\n",
    "* 로지스틱 회귀 분석에서는 입력 및 출력 계층이 있습니다. 그러나 신경망에서는 입력층과 출력층 사이에 적어도 하나의 숨은 층이 존재합니다. 이를 hidden layer 라고도 부릅니다.\n",
    "* **깊이, 내가 얼마나 많은 층을 가져야 하는지를 말하기 위해, 무엇이 깊은것 일까:** 선생님께 서 이 질문을 할 때, 그는 \"깊이\"는 상대적인 용어라고 말했습니다; 물론 그것은 얼마나 많은 숨겨진 층을 가지고 있는지를 의미하는 네트워크의 \"깊이\"를 말합니다. \"너의 수영장은 얼마나 깊니?\"가 12피트일 수도 있고, 2피트일 수도 있습니다. 그럼에도 불구하고, 그것은 여전히 깊이가 있습니다. 그것은 \"깊이\"의 질을 가지고 있다. 32년 전에는 두세 겹의 숨겨진 층을 사용했었습니다. 그것이 당시의 전문 하드웨어의 한계였습니다. 불과 몇 년 전만 해도 20개 층이 꽤 깊은 것으로 여겨졌습니다. 10월에 앤드류 응은 152개의 층이 그가 알고 있는 가장 큰 상업 네트워크 중 하나라고 언급했었습니다. \n",
    "* **Why it is called hidden:** 숨겨진 계층이 입력을 보지 못하기 때문에(train set)\n",
    "\n",
    "* 2층 신경망을 봐보겠습니다: \n",
    "<a href=\"http://ibb.co/eF315x\"><img src=\"http://preview.ibb.co/dajVyH/9.jpg\" alt=\"9\" border=\"0\"></a>\n",
    "* Step by step we will learn this image.\n",
    "    * 보다시피 입력 계층과 출력 계층 사이에는 하나의 숨겨진 계층이 있습니다. 그리고 이 숨겨진 계층은 3개의 노드를 가지고 있습니다. 노드 3의 번호를 선택하는 이유가 궁금하다면 이유는 없고, 나는 오직 개수만 선택합니다. 노드 수는 학습 속도와 같은 하이퍼 파라미터 입니다. 따라서 우리는 인공 신경망의 끝에서 하이퍼 파라미터를 보게 될 것입니다.\n",
    "    * 입력 및 출력은 변하지 않습니다. 그것들은 로지스틱 회귀와 같습니다.\n",
    "    * 이미지에는 당신에게 알려지지 않은 tanh 함수가 있습니다. 그것은 Sigmoid function과 같은 활성화 함수입니다. Tanh 활성화 함수는 hidden units 에 대해 sigmoid보다 더 좋습니다. 왜냐하면 그것의 출력의 평균은 0에 가까워서 다음 레이어에 더 잘 데이터를 배치하기 때문입니다. 또한 tanh 활성화 함수는 우리의 모델 학습을 더 좋게 하는 비선형성을 증가시킵니다.\n",
    "    * 보라색으로도 알 수 있듯이 두 부분이 있습니다. 두 부분 모두 로지스틱 회귀와 같습니다. 유일한 차이점은 활성화 함수, 입력 및 출력입니다.\n",
    "        * logistic regression: input => output\n",
    "        * 2 layer neural network: input => hidden layer => output. 숨겨진 계층은 파트 1의 출력이고 파트 2의 입력이라고 생각할 수 있습니다.\n",
    "* 그뿐입니다. 우리는 2층 신경망에 대한 로지스틱 회귀와 같은 경로를 따를 것입니다.\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35b73665-27cb-48a7-abb1-0e0e6bc10ec0",
    "_uuid": "689a4a049579fff0c718bc30639f18e2dcab8ccc"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "## 2-Layer Neural Network\n",
    "* parameters weights and bias 초기화 그리고 layers 의 size\n",
    "* Forward propagation\n",
    "* Loss function 과 Cost function\n",
    "* Backward propagation\n",
    "* Update Parameters\n",
    "* 학습된 parameters weight 와 bias 를 사용한 예측\n",
    "* Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbfe8df0-bf38-4842-aec1-c04417a99420",
    "_uuid": "0a5e286cf360d579eb6e5d5f220dd1a17c458039"
   },
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "## Size of layers and initializing parameters weights and bias\n",
    "\n",
    "\n",
    "* 샘플이 348개인 x_train의 경우 \n",
    "\n",
    "$x^{(348)}$: $$z^{[1] (348)} =  W^{[1]} x^{(348)} + b^{[1] (348)}$$ \n",
    "$$a^{[1] (348)} = \\tanh(z^{[1] (348)})$$\n",
    "$$z^{[2] (348)} = W^{[2]} a^{[1] (348)} + b^{[2] (348)}$$\n",
    "$$\\hat{y}^{(348)} = a^{[2] (348)} = \\sigma(z^{ [2] (348)})$$\n",
    "\n",
    "\n",
    "* * 로지스틱 회귀 분석에서는 weight 0.01과 bias 0을 초기화 했습니다. 이때 우리는 무작위로 weight를 초기화합니다. 왜냐하면 우리가 parameter 0을 초기화하면 첫 번째 숨겨진 층의 각 뉴런은 동일한 계산을 수행할 것이기 때문입니다. 따라서, 여러 차례 반복적으로 경사 하강 시킨 후에도 층에 있는 각 뉴런은 다른 뉴런과 같은 것들을 계산하게 될 것입니다. 따라서 무작위로 초기화합니다. 또한 초기 weight는 작을 것입니다. 초기에 크기가 매우 크면 이는 tanh의 입력을 매우 크게 하여 경사가 0에 가깝게 됩니다. 즉, 최적화 알고리즘이 느릴 것입니다.\n",
    "\n",
    "* Bias 처음부터 0이 될 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "089fd577-95a0-4218-9b53-72bf1c0ab206",
    "_uuid": "922670a74f6999885759399ebea8b10692796a29"
   },
   "outputs": [],
   "source": [
    "# parameters weights and bias 초기화 그리고 layers 의 size\n",
    "def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n",
    "    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n",
    "                  \"bias1\": np.zeros((3,1)),\n",
    "                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n",
    "                  \"bias2\": np.zeros((y_train.shape[0],1))}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65832cdf-7ee8-447b-a068-48ac2e46b49f",
    "_uuid": "66147bbafbe25dac498f3963ea8419126f624ce9"
   },
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "## Forward propagation\n",
    "* Forward propagation 는 로지스틱 회귀 분석과 거의 같습니다.\n",
    "* 단 한가지 차이점은 tanh 함수를 사용하고 모든 과정을 두 번 만든다는 것입니다.\n",
    "* 또한 numpy 는 tanh function을 가지고 있습니다. 따라서 우리는 그것을 별도로 시행할 필요가 없습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "d64d6b90-7f14-453f-a401-4119504496e3",
    "_uuid": "41e1e2f1c7afff027ba0a9f9b2fdcbe312e9a194"
   },
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation_NN(x_train, parameters):\n",
    "\n",
    "    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30a0abc9-7ee0-4093-afd5-ae9d5b1fcd5e",
    "_uuid": "ee7a42ee207e222eed5c24b1bfbf2d6ce0cdec37"
   },
   "source": [
    "<a id=\"14\"></a> <br>\n",
    "## Loss function and Cost function\n",
    "* 손실 및 비용 함수는 로지스틱 회귀 분석과 동일합니다\n",
    "* Cross entropy function (교차 엔트로피 함수)\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/nyR9LU/as.jpg\" alt=\"as\" border=\"0\"></a><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "24143d72-bc62-4f2d-b0cf-4f67a4016299",
    "_uuid": "b55887b28cffc8083a76af45d25957d4f3e9f6fa"
   },
   "outputs": [],
   "source": [
    "# Compute cost\n",
    "def compute_cost_NN(A2, Y, parameters):\n",
    "    logprobs = np.multiply(np.log(A2),Y)\n",
    "    cost = -np.sum(logprobs)/Y.shape[1]\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39839772-976a-4e53-a2e0-07aa76c9bd98",
    "_uuid": "43767f9271e2b2b6e0c3560e414f3c0c596ffe2f"
   },
   "source": [
    "<a id=\"15\"></a> <br>\n",
    "## Backward propagation\n",
    "* 알다시피 backward propagation 는 미분을 의미합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "2fcfd3c4-f935-4272-a284-c2dbc2c35afb",
    "_uuid": "6bf7bce2e4413ecdc16ea778008eee4072738aab"
   },
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "def backward_propagation_NN(parameters, cache, X, Y):\n",
    "\n",
    "    dZ2 = cache[\"A2\"]-Y\n",
    "    dW2 = np.dot(dZ2,cache[\"A1\"].T)/X.shape[1]\n",
    "    db2 = np.sum(dZ2,axis =1,keepdims=True)/X.shape[1]\n",
    "    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n",
    "    dW1 = np.dot(dZ1,X.T)/X.shape[1]\n",
    "    db1 = np.sum(dZ1,axis =1,keepdims=True)/X.shape[1]\n",
    "    grads = {\"dweight1\": dW1,\n",
    "             \"dbias1\": db1,\n",
    "             \"dweight2\": dW2,\n",
    "             \"dbias2\": db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af195fda-5649-4e6d-830e-72123f2726a8",
    "_uuid": "b1996782dc44fda7993407c9b5efee5d4fef46e4"
   },
   "source": [
    "<a id=\"16\"></a> <br>\n",
    "## Update Parameters \n",
    "* parameters 업데이트도 로지스틱 회귀 분석과 동일합니다\n",
    "* 우리는 로지스틱 회귀 분석으로 많은 작업을 수행했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "d9ae95d4-1d11-4293-822d-e1d5f0c16d1e",
    "_uuid": "facf2b475cb82e14dcc6be56b57fe2c3ad0b1a8f"
   },
   "outputs": [],
   "source": [
    "# update parameters\n",
    "def update_parameters_NN(parameters, grads, learning_rate = 0.01):\n",
    "    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n",
    "                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n",
    "                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n",
    "                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac416480-ec9c-45b4-ac9d-1caeded9ba90",
    "_uuid": "9c471502563017fabb991494359091215e4ad583"
   },
   "source": [
    "<a id=\"17\"></a> <br>\n",
    "## Prediction with learnt parameters weight and bias\n",
    "* 로지스틱 회귀 분석과 같은 예측 방법을 쓰겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "96004eb5-d6ca-41ab-a577-70fb0628a2f4",
    "_uuid": "53c00c4430c6fcc3298dde8de804cab71884caa5"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict_NN(parameters,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    A2, cache = forward_propagation_NN(x_test,parameters)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(A2.shape[1]):\n",
    "        if A2[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0df9e13-300b-4d5e-b8ec-f702ed0e06af",
    "_uuid": "94202fbc047d59fa5c8b81ba02962f1f3cc56d8f"
   },
   "source": [
    "<a id=\"18\"></a> <br>\n",
    "## Create Model\n",
    "* 그것들을 모두 합쳐보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "b66f3c28-0f71-4176-8a6b-35b98b0db936",
    "_uuid": "9babf239f800bedc9864c6a75677985d57f1cc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.368301\n",
      "Cost after iteration 100: 0.329046\n",
      "Cost after iteration 200: 0.286103\n",
      "Cost after iteration 300: 0.227233\n",
      "Cost after iteration 400: 0.172924\n",
      "Cost after iteration 500: 0.133718\n",
      "Cost after iteration 600: 0.107151\n",
      "Cost after iteration 700: 0.088694\n",
      "Cost after iteration 800: 0.075215\n",
      "Cost after iteration 900: 0.064847\n",
      "Cost after iteration 1000: 0.056493\n",
      "Cost after iteration 1100: 0.049548\n",
      "Cost after iteration 1200: 0.043699\n",
      "Cost after iteration 1300: 0.038764\n",
      "Cost after iteration 1400: 0.034601\n",
      "Cost after iteration 1500: 0.031079\n",
      "Cost after iteration 1600: 0.028085\n",
      "Cost after iteration 1700: 0.025523\n",
      "Cost after iteration 1800: 0.023315\n",
      "Cost after iteration 1900: 0.021399\n",
      "Cost after iteration 2000: 0.019726\n",
      "Cost after iteration 2100: 0.018257\n",
      "Cost after iteration 2200: 0.016961\n",
      "Cost after iteration 2300: 0.015811\n",
      "Cost after iteration 2400: 0.014787\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 100.0 %\n",
      "test accuracy: 95.16129032258064 %\n"
     ]
    }
   ],
   "source": [
    "# 2 - Layer neural network\n",
    "def two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n",
    "    cost_list = []\n",
    "    index_list = []\n",
    "    #initialize parameters and layer sizes\n",
    "    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         # forward propagation\n",
    "        A2, cache = forward_propagation_NN(x_train,parameters)\n",
    "        # compute cost\n",
    "        cost = compute_cost_NN(A2, y_train, parameters)\n",
    "         # backward propagation\n",
    "        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n",
    "         # update parameters\n",
    "        parameters = update_parameters_NN(parameters, grads)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost)\n",
    "            index_list.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    plt.plot(index_list,cost_list)\n",
    "    plt.xticks(index_list,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    \n",
    "    # predict\n",
    "    y_prediction_test = predict_NN(parameters,x_test)\n",
    "    y_prediction_train = predict_NN(parameters,x_train)\n",
    "\n",
    "    # train/test Error들을 출력합니다\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    return parameters\n",
    "\n",
    "parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04d3b704-2fd4-410b-ad90-a5a0aedd4b7d",
    "_uuid": "62bb69efcf883ba22581dfc8d4f2e0eaa2d99ee3"
   },
   "source": [
    "<font color='purple'>\n",
    "여기까지 우리는 2층 신경망을 만들고 구현하는 방법을 배웠습니다.\n",
    "* parameters weights and bias 초기화 그리고 layers 의 size\n",
    "* Forward propagation\n",
    "* Loss function 과 Cost function\n",
    "* Backward propagation\n",
    "* Update Parameters\n",
    "* 학습된 parameters weight 와 bias 를 사용한 예측\n",
    "* Create Model\n",
    "\n",
    "<br> 이제 keras 로 L층 신경망을 구현하는 방법을 배워보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c054ed2a-9dd9-498c-9be6-6e9b16ff5913",
    "_uuid": "aa2f896b72236e09687afc3c613a4fc801b16552"
   },
   "source": [
    "<a id=\"19\"></a> <br>\n",
    "# L Layer Neural Network\n",
    "* **hidden layer가 증가하면 어떻게 될까요?:** 이전 layer들은 간단한 feature들을 감지할 수 있습니다.\n",
    "* 간단한 feature를 후기 신경망 층에서 함께 모델화 하여 더욱 복잡한 function을 학습할 수 있습니다. 예를들어 우리의 간단한 예를 봅시다.(sign1)\n",
    "<a href=\"http://ibb.co/dNgDJH\"><img src=\"http://preview.ibb.co/mpD4Qx/10.jpg\" alt=\"10\" border=\"0\"></a>\n",
    "* 예를 들어, 첫 번째 숨겨진 층은 가장자리나 선과 같은 기본 모양을 학습합니다. 층수가 증가하면, 층들은 볼록한 모양이나 집게손가락과 같은 특징과 같은 더 복잡한 것들을 배우기 시작합니다.\n",
    "* 모델을 생성해 봅시다\n",
    "    * 학습 속도, 반복 횟수, 숨겨진 계층 수, 숨겨진 단위 수, 활성화 함수 유형 등 우리가 선택해야 하는 일부 하이퍼 파라미터가 있습니다(earning rate, number of iterations, number of hidden layer, number of hidden units, type of activation functions)\n",
    "    * 이러한 하이퍼 파라미터는 딥러닝 세계에서 많은 시간을 보낸다면 연속적으로 선택될 수 있습니다\n",
    "    * 이 튜토리얼에서 우리 모델은 각각 8개 노드와 4개의 노드를 가진 2개의 숨겨진 계층을 가질 것입니다. 숨겨진 계층과 노드의 수가 증가하면 시간이 너무 많이 걸리기 때문입니다.\n",
    "    * 활성화 기능으로 각각 relu(첫 번째 숨김 계층), relu(두 번째 숨김 계층), sigmoid(출력 계층)를 사용합니다.\n",
    "    * 반복 횟수는 100회 입니다.(iteration )\n",
    "* 우리의 방법은 이전 부분과 동일하지만, 당신이 딥러닝의 이면에 있는 논리를 배우면, 우리는 우리의 일을 쉽게 할 수 있고 더 깊은 신경망을 위해 케라스 라이브러리를 사용할 수 있습니다.\n",
    "* 먼저 우리의 x_train, x_test, y_train and y_test 들을 reshape 합니다\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "631a05c4-e362-4fa0-9048-21c599f55344",
    "_uuid": "0a978924a68d423de4babe73c15412ad938c1858"
   },
   "outputs": [],
   "source": [
    "# reshaping\n",
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e17b5a34-00dc-49a8-b7ac-a2bd3a753f78",
    "_uuid": "5a78a5570bc50180a02190dee46180f19eb165e1"
   },
   "source": [
    "<a id=\"22\"></a> <br>\n",
    "## Implementing with keras library\n",
    "케라스 라이브러리의 몇 가지 parameter를 살펴보십시오.:\n",
    "* units: output dimensions of node\n",
    "* kernel_initializer: to initialize weights\n",
    "* activation: activation function, 여기서 우리는 relu 를 사용합니다\n",
    "* input_dim: 이미지의 픽셀 수인 입력 치수(4096 px)\n",
    "* optimizer: 우리는 adam optimizer 를 사용합니다\n",
    "    * Adam 은 신경망 훈련을 위한 가장 효과적인 최적화 알고리즘 중 하나입니다.\n",
    "    * Adam의 몇 가지 장점은 상대적으로 낮은 메모리 요구 사항과 일반적으로 하이퍼 파라미터의 조정이 거의 없더라도 잘 작동한다는 것입니다.\n",
    "* loss: Cost function 과 같습니다. \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "* metrics: it is accuracy.\n",
    "* cross_val_score: 교차검증을 사용합니다. 차 검증을 모를 경우 기계 학습 튜토리얼에서 확인하십시오. https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n",
    "* epochs: number of iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "8870c45b-b0fe-4050-a8af-41498a417ed5",
    "_uuid": "9361c3183a40fa0080055b7d5c1002aef68b4d77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.4526\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6927 - accuracy: 0.5431\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 108us/step - loss: 0.6924 - accuracy: 0.5431\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.6918 - accuracy: 0.5474\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.6905 - accuracy: 0.5603\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6887 - accuracy: 0.8147\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.6866 - accuracy: 0.6983\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6814 - accuracy: 0.7543\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6746 - accuracy: 0.8405\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.6667 - accuracy: 0.8405\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.6574 - accuracy: 0.8017\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6445 - accuracy: 0.8276\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.6365 - accuracy: 0.6509\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.6232 - accuracy: 0.7802\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.6019 - accuracy: 0.8233\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.5869 - accuracy: 0.7974\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.5702 - accuracy: 0.8448\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.5538 - accuracy: 0.7759\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.5494 - accuracy: 0.8578\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.5230 - accuracy: 0.8664\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.5093 - accuracy: 0.8750\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.5008 - accuracy: 0.9009\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.4829 - accuracy: 0.8750\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.4727 - accuracy: 0.8966\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.4603 - accuracy: 0.8707\n",
      "Epoch 26/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.4660 - accuracy: 0.8879\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.4420 - accuracy: 0.8922\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.4522 - accuracy: 0.9310\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.4400 - accuracy: 0.9267\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.4172 - accuracy: 0.9181\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.4096 - accuracy: 0.9267\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.4118 - accuracy: 0.9052\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.3969 - accuracy: 0.9138\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.3878 - accuracy: 0.9397\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.3854 - accuracy: 0.9440\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.3764 - accuracy: 0.9440\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3747 - accuracy: 0.9353\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.3680 - accuracy: 0.9224\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3676 - accuracy: 0.9440\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.3761 - accuracy: 0.8966\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.3635 - accuracy: 0.9267\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.3648 - accuracy: 0.9181\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3526 - accuracy: 0.9267\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3382 - accuracy: 0.9440\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 0s 159us/step - loss: 0.3234 - accuracy: 0.9483\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 0s 164us/step - loss: 0.3226 - accuracy: 0.9526\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.3131 - accuracy: 0.9526\n",
      "Epoch 48/100\n",
      "232/232 [==============================] - 0s 164us/step - loss: 0.3121 - accuracy: 0.9569\n",
      "Epoch 49/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3059 - accuracy: 0.9526\n",
      "Epoch 50/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3102 - accuracy: 0.9397\n",
      "Epoch 51/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.3183 - accuracy: 0.9397\n",
      "Epoch 52/100\n",
      "232/232 [==============================] - 0s 155us/step - loss: 0.3023 - accuracy: 0.9397\n",
      "Epoch 53/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.2870 - accuracy: 0.9655\n",
      "Epoch 54/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.2925 - accuracy: 0.9612\n",
      "Epoch 55/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2867 - accuracy: 0.9440\n",
      "Epoch 56/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2966 - accuracy: 0.9397\n",
      "Epoch 57/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.2851 - accuracy: 0.9440\n",
      "Epoch 58/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.2695 - accuracy: 0.9440\n",
      "Epoch 59/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.2592 - accuracy: 0.9698\n",
      "Epoch 60/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2653 - accuracy: 0.9569\n",
      "Epoch 61/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.2887 - accuracy: 0.9440\n",
      "Epoch 62/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.3006 - accuracy: 0.9267\n",
      "Epoch 63/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.2679 - accuracy: 0.9569\n",
      "Epoch 64/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.2413 - accuracy: 0.9655\n",
      "Epoch 65/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2368 - accuracy: 0.9741\n",
      "Epoch 66/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.2308 - accuracy: 0.9741\n",
      "Epoch 67/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2258 - accuracy: 0.9741\n",
      "Epoch 68/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.2222 - accuracy: 0.9741\n",
      "Epoch 69/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.2194 - accuracy: 0.9741\n",
      "Epoch 70/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.2210 - accuracy: 0.9784\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 0s 129us/step - loss: 0.2163 - accuracy: 0.9698\n",
      "Epoch 72/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.2259 - accuracy: 0.9655\n",
      "Epoch 73/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.2239 - accuracy: 0.9569\n",
      "Epoch 74/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.2030 - accuracy: 0.9784\n",
      "Epoch 75/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.2025 - accuracy: 0.9741\n",
      "Epoch 76/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.2014 - accuracy: 0.9655\n",
      "Epoch 77/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1949 - accuracy: 0.9741\n",
      "Epoch 78/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1872 - accuracy: 0.9741\n",
      "Epoch 79/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1871 - accuracy: 0.9741\n",
      "Epoch 80/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1812 - accuracy: 0.9784\n",
      "Epoch 81/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1789 - accuracy: 0.9741\n",
      "Epoch 82/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1803 - accuracy: 0.9698\n",
      "Epoch 83/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1773 - accuracy: 0.9784\n",
      "Epoch 84/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.1939 - accuracy: 0.9655\n",
      "Epoch 85/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1722 - accuracy: 0.9741\n",
      "Epoch 86/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1774 - accuracy: 0.9655\n",
      "Epoch 87/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.1690 - accuracy: 0.9741\n",
      "Epoch 88/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1586 - accuracy: 0.9828\n",
      "Epoch 89/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1589 - accuracy: 0.9828\n",
      "Epoch 90/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1755 - accuracy: 0.9741\n",
      "Epoch 91/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1659 - accuracy: 0.9741\n",
      "Epoch 92/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1496 - accuracy: 0.9914\n",
      "Epoch 93/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1483 - accuracy: 0.9828\n",
      "Epoch 94/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1608 - accuracy: 0.9655\n",
      "Epoch 95/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.1395 - accuracy: 0.9784\n",
      "Epoch 96/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1360 - accuracy: 0.9828\n",
      "Epoch 97/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1389 - accuracy: 0.9784\n",
      "Epoch 98/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.1341 - accuracy: 0.9784\n",
      "Epoch 99/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.1346 - accuracy: 0.9784\n",
      "Epoch 100/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1291 - accuracy: 0.9871\n",
      "116/116 [==============================] - 0s 336us/step\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 0s 840us/step - loss: 0.6932 - accuracy: 0.5216\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.6919 - accuracy: 0.5216\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.6908 - accuracy: 0.5216\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.6894 - accuracy: 0.5216\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 177us/step - loss: 0.6875 - accuracy: 0.5216\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6852 - accuracy: 0.5216\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6822 - accuracy: 0.5216\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 224us/step - loss: 0.6789 - accuracy: 0.5216\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6695 - accuracy: 0.5216\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6610 - accuracy: 0.5216\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.6507 - accuracy: 0.5216\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.6362 - accuracy: 0.5216\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.6209 - accuracy: 0.7543\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.5993 - accuracy: 0.7198\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.5765 - accuracy: 0.8233\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.5438 - accuracy: 0.9138\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.5080 - accuracy: 0.8879\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.4763 - accuracy: 0.9009\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.4367 - accuracy: 0.9009\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 0s 159us/step - loss: 0.3989 - accuracy: 0.9052\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.3696 - accuracy: 0.9310\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 0s 198us/step - loss: 0.3320 - accuracy: 0.9310\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 0s 237us/step - loss: 0.3049 - accuracy: 0.9440\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.2993 - accuracy: 0.9267\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.2798 - accuracy: 0.9353\n",
      "Epoch 26/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.2723 - accuracy: 0.9224\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.2400 - accuracy: 0.9267\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 0s 172us/step - loss: 0.2297 - accuracy: 0.9483\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.2053 - accuracy: 0.9526\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1948 - accuracy: 0.9526\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1924 - accuracy: 0.9483\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1747 - accuracy: 0.9612\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1703 - accuracy: 0.9655\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1705 - accuracy: 0.9397\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.1792 - accuracy: 0.9353\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1607 - accuracy: 0.9612\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1591 - accuracy: 0.9310\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1392 - accuracy: 0.9655\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1382 - accuracy: 0.9569\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1474 - accuracy: 0.9569\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1225 - accuracy: 0.9741\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1214 - accuracy: 0.9655\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1143 - accuracy: 0.9741\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.1163 - accuracy: 0.9698\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.1130 - accuracy: 0.9784\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1300 - accuracy: 0.9612\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1213 - accuracy: 0.9655\n",
      "Epoch 48/100\n",
      "232/232 [==============================] - 0s 159us/step - loss: 0.1102 - accuracy: 0.9612\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 0s 125us/step - loss: 0.1156 - accuracy: 0.9655\n",
      "Epoch 50/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1023 - accuracy: 0.9741\n",
      "Epoch 51/100\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.96 - 0s 138us/step - loss: 0.0968 - accuracy: 0.9784\n",
      "Epoch 52/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.1004 - accuracy: 0.9698\n",
      "Epoch 53/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0941 - accuracy: 0.9741\n",
      "Epoch 54/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0873 - accuracy: 0.9784\n",
      "Epoch 55/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0842 - accuracy: 0.9741\n",
      "Epoch 56/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0869 - accuracy: 0.9784\n",
      "Epoch 57/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0807 - accuracy: 0.9784\n",
      "Epoch 58/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0860 - accuracy: 0.9698\n",
      "Epoch 59/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0721 - accuracy: 0.9828\n",
      "Epoch 60/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0726 - accuracy: 0.9784\n",
      "Epoch 61/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0878 - accuracy: 0.9741\n",
      "Epoch 62/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.0716 - accuracy: 0.9828\n",
      "Epoch 63/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0695 - accuracy: 0.9828\n",
      "Epoch 64/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0665 - accuracy: 0.9828\n",
      "Epoch 65/100\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.96 - 0s 138us/step - loss: 0.0676 - accuracy: 0.9784\n",
      "Epoch 66/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0712 - accuracy: 0.9828\n",
      "Epoch 67/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0647 - accuracy: 0.9828\n",
      "Epoch 68/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0628 - accuracy: 0.9871\n",
      "Epoch 69/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0582 - accuracy: 0.9828\n",
      "Epoch 70/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0581 - accuracy: 0.9828\n",
      "Epoch 71/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0711 - accuracy: 0.9828\n",
      "Epoch 72/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0596 - accuracy: 0.9828\n",
      "Epoch 73/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0531 - accuracy: 0.9828\n",
      "Epoch 74/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0646 - accuracy: 0.9914\n",
      "Epoch 75/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0709 - accuracy: 0.9828\n",
      "Epoch 76/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0735 - accuracy: 0.9698\n",
      "Epoch 77/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0736 - accuracy: 0.9698\n",
      "Epoch 78/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0797 - accuracy: 0.9655\n",
      "Epoch 79/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0668 - accuracy: 0.9828\n",
      "Epoch 80/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0669 - accuracy: 0.9828\n",
      "Epoch 81/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0610 - accuracy: 0.9784\n",
      "Epoch 82/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0532 - accuracy: 0.9871\n",
      "Epoch 83/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0448 - accuracy: 0.9914\n",
      "Epoch 84/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0423 - accuracy: 0.9914\n",
      "Epoch 85/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0396 - accuracy: 0.9957\n",
      "Epoch 86/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0387 - accuracy: 0.9957\n",
      "Epoch 87/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0471 - accuracy: 0.9871\n",
      "Epoch 88/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0403 - accuracy: 0.9871\n",
      "Epoch 89/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0385 - accuracy: 0.9914\n",
      "Epoch 90/100\n",
      "232/232 [==============================] - 0s 233us/step - loss: 0.0356 - accuracy: 0.9957\n",
      "Epoch 91/100\n",
      "232/232 [==============================] - 0s 293us/step - loss: 0.0363 - accuracy: 0.9871\n",
      "Epoch 92/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0348 - accuracy: 0.9957\n",
      "Epoch 93/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0335 - accuracy: 0.9957\n",
      "Epoch 94/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0338 - accuracy: 0.9957\n",
      "Epoch 95/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0337 - accuracy: 0.9914\n",
      "Epoch 96/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0317 - accuracy: 0.9957\n",
      "Epoch 97/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0317 - accuracy: 0.9957\n",
      "Epoch 98/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0476 - accuracy: 0.9914\n",
      "Epoch 99/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0391 - accuracy: 0.9914\n",
      "Epoch 100/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0283 - accuracy: 1.0000\n",
      "116/116 [==============================] - 0s 612us/step\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 0s 926us/step - loss: 0.6932 - accuracy: 0.4957\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6930 - accuracy: 0.4957\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6922 - accuracy: 0.4957\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 108us/step - loss: 0.6910 - accuracy: 0.6466\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6880 - accuracy: 0.6164\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6828 - accuracy: 0.5345\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 108us/step - loss: 0.6738 - accuracy: 0.8147\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.6614 - accuracy: 0.5216\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.6348 - accuracy: 0.8405\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.6050 - accuracy: 0.8448\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.5601 - accuracy: 0.8793\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.5115 - accuracy: 0.8707\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.4706 - accuracy: 0.8448\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.4393 - accuracy: 0.8750\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.3962 - accuracy: 0.8879\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3515 - accuracy: 0.8922\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3389 - accuracy: 0.8879\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.3092 - accuracy: 0.9009\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.2899 - accuracy: 0.9095\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.2640 - accuracy: 0.9138\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2470 - accuracy: 0.9267\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.2335 - accuracy: 0.9224\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.2171 - accuracy: 0.9440\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.2077 - accuracy: 0.9224\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 0s 112us/step - loss: 0.1945 - accuracy: 0.9397\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 0s 121us/step - loss: 0.1847 - accuracy: 0.9397\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1783 - accuracy: 0.9353\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 0s 362us/step - loss: 0.1673 - accuracy: 0.9440\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.1637 - accuracy: 0.9526\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.1649 - accuracy: 0.9440\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 1.00 - 0s 151us/step - loss: 0.1605 - accuracy: 0.9440\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1527 - accuracy: 0.9483\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1405 - accuracy: 0.9483\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1431 - accuracy: 0.9526\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.1413 - accuracy: 0.9569\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1389 - accuracy: 0.9440\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1141 - accuracy: 0.9612\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1123 - accuracy: 0.9655\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1115 - accuracy: 0.9741\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.1117 - accuracy: 0.9655\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1050 - accuracy: 0.9655\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.1025 - accuracy: 0.9655\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.1074 - accuracy: 0.9784\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.0986 - accuracy: 0.9655\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.0940 - accuracy: 0.9698\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0930 - accuracy: 0.9784\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.1092 - accuracy: 0.9569\n",
      "Epoch 48/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0997 - accuracy: 0.9569\n",
      "Epoch 49/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0852 - accuracy: 0.9741\n",
      "Epoch 50/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0876 - accuracy: 0.9871\n",
      "Epoch 51/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.1158 - accuracy: 0.9569\n",
      "Epoch 52/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0930 - accuracy: 0.9784\n",
      "Epoch 53/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.1202 - accuracy: 0.9483\n",
      "Epoch 54/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.1531 - accuracy: 0.9310\n",
      "Epoch 55/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.1214 - accuracy: 0.9569\n",
      "Epoch 56/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0769 - accuracy: 0.9784\n",
      "Epoch 57/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0804 - accuracy: 0.9698\n",
      "Epoch 58/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0698 - accuracy: 0.9784\n",
      "Epoch 59/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0679 - accuracy: 0.9784\n",
      "Epoch 60/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0844 - accuracy: 0.9698\n",
      "Epoch 61/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0645 - accuracy: 0.9871\n",
      "Epoch 62/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0726 - accuracy: 0.9698\n",
      "Epoch 63/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0661 - accuracy: 0.9871\n",
      "Epoch 64/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0638 - accuracy: 0.9784\n",
      "Epoch 65/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0628 - accuracy: 0.9784\n",
      "Epoch 66/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0615 - accuracy: 0.9871\n",
      "Epoch 67/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0572 - accuracy: 0.9828\n",
      "Epoch 68/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0651 - accuracy: 0.9784\n",
      "Epoch 69/100\n",
      "232/232 [==============================] - 0s 142us/step - loss: 0.0554 - accuracy: 0.9914\n",
      "Epoch 70/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0781 - accuracy: 0.9741\n",
      "Epoch 71/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0972 - accuracy: 0.9612\n",
      "Epoch 72/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0640 - accuracy: 0.9784\n",
      "Epoch 73/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0596 - accuracy: 0.9828\n",
      "Epoch 74/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0563 - accuracy: 0.9914\n",
      "Epoch 75/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0560 - accuracy: 0.9871\n",
      "Epoch 76/100\n",
      "232/232 [==============================] - 0s 146us/step - loss: 0.0511 - accuracy: 0.9871\n",
      "Epoch 77/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0553 - accuracy: 0.9784\n",
      "Epoch 78/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0563 - accuracy: 0.9871\n",
      "Epoch 79/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0556 - accuracy: 0.9828\n",
      "Epoch 80/100\n",
      "232/232 [==============================] - 0s 116us/step - loss: 0.0562 - accuracy: 0.9828\n",
      "Epoch 81/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0498 - accuracy: 0.9914\n",
      "Epoch 82/100\n",
      "232/232 [==============================] - 0s 121us/step - loss: 0.0488 - accuracy: 0.9871\n",
      "Epoch 83/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0968 - accuracy: 0.9698\n",
      "Epoch 84/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0607 - accuracy: 0.9784\n",
      "Epoch 85/100\n",
      "232/232 [==============================] - 0s 185us/step - loss: 0.0477 - accuracy: 0.9914\n",
      "Epoch 86/100\n",
      "232/232 [==============================] - 0s 151us/step - loss: 0.0494 - accuracy: 0.9871\n",
      "Epoch 87/100\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.0488 - accuracy: 0.9914\n",
      "Epoch 88/100\n",
      "232/232 [==============================] - 0s 138us/step - loss: 0.0819 - accuracy: 0.9698\n",
      "Epoch 89/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0495 - accuracy: 0.9871\n",
      "Epoch 90/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0559 - accuracy: 0.9828\n",
      "Epoch 91/100\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.96 - 0s 142us/step - loss: 0.0584 - accuracy: 0.9741\n",
      "Epoch 92/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0362 - accuracy: 0.9914\n",
      "Epoch 93/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0365 - accuracy: 0.9871\n",
      "Epoch 94/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0431 - accuracy: 0.9914\n",
      "Epoch 95/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0372 - accuracy: 0.9914\n",
      "Epoch 96/100\n",
      "232/232 [==============================] - 0s 125us/step - loss: 0.0355 - accuracy: 0.9957\n",
      "Epoch 97/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0432 - accuracy: 0.9914\n",
      "Epoch 98/100\n",
      "232/232 [==============================] - 0s 134us/step - loss: 0.0432 - accuracy: 0.9828\n",
      "Epoch 99/100\n",
      "232/232 [==============================] - 0s 164us/step - loss: 0.0383 - accuracy: 0.9914\n",
      "Epoch 100/100\n",
      "232/232 [==============================] - 0s 129us/step - loss: 0.0357 - accuracy: 0.9871\n",
      "116/116 [==============================] - 0s 689us/step\n",
      "Accuracy mean: 0.9511494239171346\n",
      "Accuracy variance: 0.01625530892527764\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # 신경망 라이브러리를 초기화합니다.\n",
    "from keras.layers import Dense \n",
    "def build_classifier():\n",
    "    classifier = Sequential() # neural network 를 초기화 합니다\n",
    "    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3963de5c-1b2e-45ec-b499-4c404afa8595",
    "_uuid": "a73620b9d04cd7627e7da9961db541140dfd8467"
   },
   "source": [
    "<a id=\"23\"></a> <br>\n",
    "## Artificial Neural Network with Pytorch library.\n",
    "* Pytorch 는 keras 와 같은 프레임 중 하나입니다.\n",
    "* 딥러닝의 구현 및 구축이 편리합니다.  \n",
    "* Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a62d8b1-8308-4654-bdfd-479965ee08af",
    "_uuid": "dc3f380e7953a5c3bb6580b30c0da7cb94599c81"
   },
   "source": [
    "<a id=\"20\"></a> <br>\n",
    "# Conclusion\n",
    "* 먼저 우리는 이 데이터 세트에 감사합니다.\n",
    "* 맞춤법이 틀린것들은(너무 많이 틀릴 수 있음) 무시해주세요 :)\n",
    "* 이 튜토리얼은 얕으며, 일부 개념에 대한 자세한 내용을 원하면 코멘트를 할 수 있습니다.\n",
    "* 내가 몇 가지 개념을 이해할 수 없다고 생각되면 유튜브(특히 앤드류)에서 배운 후 계속 진행하십시오.\n",
    "* Python 또는 Machine Learning과 관련된 내용을 이해하지 못하는 경우 다른 튜토리얼을 확인하십시오.\n",
    "    * Data Science: https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners\n",
    "    * Machine learning: https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n",
    "* 이제 나는 당신이 deep learning 이 무엇인지 이해하고 배우기를 바랍니다. 그러나 우리는 딥러닝 모델을 구축하기 위해 매번 긴 코드를 쓰지 않습니다. 따라서 딥러닝 모델을 빠르고 쉽게 구축하기 위한 딥러닝 프레임 작업이 있습니다.\n",
    "    * Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n",
    "    * Convolutional Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n",
    "    * Recurrent Neural Network: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch\n",
    "     \n",
    "\n",
    "# 감사합니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
